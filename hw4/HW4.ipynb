{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gB0P8T4zgUf"
      },
      "source": [
        "# Original classifier (Copy from slide 882)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-hFzdvoSn-G",
        "outputId": "1ca281ac-74fa-477b-a006-a6eab67c4009"
      },
      "source": [
        "import numpy as np, sys\n",
        "np.random.seed(1)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "images, labels = (x_train[0:1000].reshape(1000, 28*28)/255, y_train[0:1000])\n",
        "\n",
        "one_hot_labels = np.zeros((len(labels), 10))\n",
        "\n",
        "for i, l in enumerate(labels):\n",
        "  one_hot_labels[i][l] = 1\n",
        "\n",
        "labels = one_hot_labels\n",
        "\n",
        "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
        "test_labels = np.zeros((len(y_test), 10))\n",
        "\n",
        "for i, l in enumerate(y_test):\n",
        "  test_labels[i][l] = 1\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh2deriv(output):\n",
        "  return 1 - (output ** 2)\n",
        "\n",
        "def softmax(x):\n",
        "  temp = np.exp(x)\n",
        "  return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "alpha, iterations, hidden_size = (2, 300, 100)\n",
        "pixels_per_image, num_labels = (784, 10)\n",
        "batch_size = 100\n",
        "\n",
        "weights_0_1 = 0.02 * np.random.random((pixels_per_image, hidden_size)) - 0.01\n",
        "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
        "\n",
        "relu = lambda x:(x >= 0) * x\n",
        "relu2deriv = lambda x: (x >= 0)\n",
        "\n",
        "for j in range(iterations):\n",
        "  correct_cnt = 0\n",
        "  for i in range(int(len(images) / batch_size)):\n",
        "    batch_start, batch_end = ((i * batch_size), ((i + 1) * batch_size))\n",
        "    layer_0 = images[batch_start:batch_end]\n",
        "    layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
        "    dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
        "    layer_1 *= dropout_mask * 2\n",
        "    layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
        "\n",
        "    for k in range(batch_size):\n",
        "      correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start + k: batch_start + k + 1]))\n",
        "    layer_2_delta = (labels[batch_start:batch_end] - layer_2) /(batch_size * layer_2.shape[0])\n",
        "    layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
        "    layer_1_delta *= dropout_mask\n",
        "\n",
        "    weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "    weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "  test_correct_cnt = 0\n",
        "\n",
        "  for i in range(len(test_images)):\n",
        "    layer_0 = test_images[i: i+ 1]\n",
        "    layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
        "    layer_2 = np.dot(layer_1, weights_1_2)\n",
        "    test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
        "  \n",
        "  if (j % 10 == 0):\n",
        "    sys.stdout.write(\"\\n\" + \"I:\" + str(j) + \" Test-Acc:\" + str(test_correct_cnt/float(len(test_images))) + \" Train-Acc:\" + str(correct_cnt/float(len(images))))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "\n",
            "I:0 Test-Acc:0.394 Train-Acc:0.156\n",
            "I:10 Test-Acc:0.6867 Train-Acc:0.723\n",
            "I:20 Test-Acc:0.7025 Train-Acc:0.732\n",
            "I:30 Test-Acc:0.734 Train-Acc:0.763\n",
            "I:40 Test-Acc:0.7663 Train-Acc:0.794\n",
            "I:50 Test-Acc:0.7913 Train-Acc:0.819\n",
            "I:60 Test-Acc:0.8102 Train-Acc:0.849\n",
            "I:70 Test-Acc:0.8228 Train-Acc:0.864\n",
            "I:80 Test-Acc:0.831 Train-Acc:0.867\n",
            "I:90 Test-Acc:0.8364 Train-Acc:0.885\n",
            "I:100 Test-Acc:0.8407 Train-Acc:0.883\n",
            "I:110 Test-Acc:0.845 Train-Acc:0.891\n",
            "I:120 Test-Acc:0.8481 Train-Acc:0.901\n",
            "I:130 Test-Acc:0.8505 Train-Acc:0.901\n",
            "I:140 Test-Acc:0.8526 Train-Acc:0.905\n",
            "I:150 Test-Acc:0.8555 Train-Acc:0.914\n",
            "I:160 Test-Acc:0.8577 Train-Acc:0.925\n",
            "I:170 Test-Acc:0.8596 Train-Acc:0.918\n",
            "I:180 Test-Acc:0.8619 Train-Acc:0.933\n",
            "I:190 Test-Acc:0.863 Train-Acc:0.933\n",
            "I:200 Test-Acc:0.8642 Train-Acc:0.926\n",
            "I:210 Test-Acc:0.8653 Train-Acc:0.931\n",
            "I:220 Test-Acc:0.8668 Train-Acc:0.93\n",
            "I:230 Test-Acc:0.8672 Train-Acc:0.937\n",
            "I:240 Test-Acc:0.8681 Train-Acc:0.938\n",
            "I:250 Test-Acc:0.8687 Train-Acc:0.937\n",
            "I:260 Test-Acc:0.8684 Train-Acc:0.945\n",
            "I:270 Test-Acc:0.8703 Train-Acc:0.951\n",
            "I:280 Test-Acc:0.8699 Train-Acc:0.949\n",
            "I:290 Test-Acc:0.8701 Train-Acc:0.94"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF4BcHJMSZrL"
      },
      "source": [
        "# Modified Classifier (Part 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AT-_F7Hznet",
        "outputId": "9d5f66aa-8834-49f9-e681-4f9318a9787f"
      },
      "source": [
        "import numpy as np, sys\n",
        "np.random.seed(1)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "# With data augmentation to prevent overfitting (accuracy 0.99286)\n",
        "\n",
        "# Part C\n",
        "def augment_images() :\n",
        "  gen = ImageDataGenerator(\n",
        "    rotation_range=10,  \n",
        "    width_shift_range=0.1,  \n",
        "    height_shift_range=0.1,\n",
        "  )  \n",
        "\n",
        "  (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "  # reshape to be [samples][width][height][channels]\n",
        "  X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
        "\n",
        "  gen = gen.flow(X_train, y_train)\n",
        "\n",
        "  # configure batch size and retrieve one batch of images\n",
        "  data_list_x = []\n",
        "  data_list_y = []\n",
        "  batch_index = 0\n",
        "\n",
        "  while batch_index <= gen.batch_index:\n",
        "      data_x, data_y = gen.next()\n",
        "      data_list_x.append(data_x[0])\n",
        "      data_list_y.append(data_y[0])\n",
        "      batch_index = batch_index + 1\n",
        "\n",
        "  # now, data_array is the numeric data of whole images\n",
        "  x_aug_train = np.asarray(data_list_x)\n",
        "  X_train = np.append(X_train, x_aug_train, axis=0)\n",
        "  X_train = X_train.reshape((X_train.shape[0], 28, 28))\n",
        "\n",
        "  y_aug_train = np.asarray(data_list_y)\n",
        "\n",
        "  y_train = np.append(y_train, y_aug_train, axis=0)\n",
        "\n",
        "  # sys.exit()\n",
        "  return (X_train, y_train), (X_test, y_test) \n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = augment_images()\n",
        "\n",
        "# Part F\n",
        "scaler = StandardScaler()\n",
        "# fit and transform in one step\n",
        "x_train = x_train.reshape((x_train.shape[0], 28 * 28))\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "# inverse transform\n",
        "x_train = scaler.inverse_transform(x_train)\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28))\n",
        "\n",
        "images, labels = (x_train[0:1000].reshape(1000, 28*28)/255, y_train[0:1000])\n",
        "\n",
        "one_hot_labels = np.zeros((len(labels), 10))\n",
        "\n",
        "for i, l in enumerate(labels):\n",
        "  one_hot_labels[i][l] = 1\n",
        "\n",
        "labels = one_hot_labels\n",
        "\n",
        "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
        "test_labels = np.zeros((len(y_test), 10))\n",
        "\n",
        "for i, l in enumerate(y_test):\n",
        "  test_labels[i][l] = 1\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh2deriv(output):\n",
        "  return 1 - (output ** 2)\n",
        "\n",
        "def softmax(x):\n",
        "  temp = np.exp(x)\n",
        "  return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "# Part G\n",
        "alpha, iterations, hidden_size = (2, 300, 100)\n",
        "# alpha, iterations, hidden_size = (2, 500, 200)\n",
        "pixels_per_image, num_labels = (784, 10)\n",
        "batch_size = 100\n",
        "# batch_size = 200\n",
        "\n",
        "weights_0_1 = 0.02 * np.random.random((pixels_per_image, hidden_size)) - 0.01\n",
        "weights_1_2 = 0.2 * np.random.random((hidden_size, hidden_size)) - 0.1\n",
        "weights_2_3 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
        "\n",
        "relu = lambda x:(x >= 0) * x\n",
        "relu2deriv = lambda x: (x >= 0)\n",
        "\n",
        "for j in range(iterations):\n",
        "  correct_cnt = 0\n",
        "  # Part A\n",
        "  for i in range(int(len(images) / batch_size)):\n",
        "    batch_start, batch_end = ((i * batch_size), ((i + 1) * batch_size))\n",
        "    layer_0 = images[batch_start:batch_end]\n",
        "    \n",
        "    # Part E\n",
        "    layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
        "    # Part B\n",
        "    dropout_mask_1 = np.random.randint(2, size=layer_1.shape)\n",
        "    layer_1 *= dropout_mask_1 * 2\n",
        "    \n",
        "    # Part E\n",
        "    layer_2 = relu(np.dot(layer_1, weights_1_2))\n",
        "    # Part B\n",
        "    dropout_mask_2 = np.random.randint(2, size=layer_2.shape)\n",
        "    layer_2 *= dropout_mask_2 * 2\n",
        "\n",
        "    # Part D\n",
        "    layer_3 = softmax(np.dot(layer_2, weights_2_3))\n",
        "\n",
        "    for k in range(batch_size):\n",
        "      correct_cnt += int(np.argmax(layer_3[k:k+1]) == np.argmax(labels[batch_start + k: batch_start + k + 1]))\n",
        "    \n",
        "    layer_3_delta = (labels[batch_start:batch_end] - layer_3) /(batch_size * layer_3.shape[0])\n",
        "    \n",
        "    layer_2_delta = layer_3_delta.dot(weights_2_3.T) * relu2deriv(layer_2)\n",
        "    layer_2_delta *= dropout_mask_2\n",
        "\n",
        "    layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
        "    layer_1_delta *= dropout_mask_1\n",
        "\n",
        "    weights_2_3 += alpha * layer_2.T.dot(layer_3_delta)\n",
        "    \n",
        "    weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "    \n",
        "    weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "  test_correct_cnt = 0\n",
        "  total_test_correct_cnt = []\n",
        "  total_actual_correct_cnt = []\n",
        "  for i in range(len(test_images)):\n",
        "    layer_0 = test_images[i: i+ 1]\n",
        "    layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "    layer_2 = relu(np.dot(layer_1, weights_1_2))\n",
        "    layer_3 = np.dot(layer_2, weights_2_3)\n",
        "    test_correct_cnt += int(np.argmax(layer_3) == np.argmax(test_labels[i:i+1]))\n",
        "    if int(np.argmax(layer_3) == np.argmax(test_labels[i:i+1])) == 1:\n",
        "      total_test_correct_cnt.append(1)\n",
        "      total_actual_correct_cnt.append(1)\n",
        "    else:\n",
        "      total_test_correct_cnt.append(0)\n",
        "      total_actual_correct_cnt.append(1)\n",
        "  if (j % 10 == 0):\n",
        "    # Part H\n",
        "    sys.stdout.write(\"\\n\" + \"I:\" + str(j) + \" Test-Acc:\" + str(test_correct_cnt/float(len(test_images))) + \" Train-Acc:\" + str(correct_cnt/float(len(images))))\n",
        "\n",
        "    total_test_correct_cnt = np.asarray(total_test_correct_cnt)\n",
        "    \n",
        "    print(\"\\n\",f1_score(total_actual_correct_cnt, total_test_correct_cnt, average=None), \"\\n\")\n",
        "# Part I not sure what top common errors would be here when the code is checking it."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "I:0 Test-Acc:0.0825 Train-Acc:0.09\n",
            " [0.         0.15242494] \n",
            "\n",
            "\n",
            "I:10 Test-Acc:0.4592 Train-Acc:0.376\n",
            " [0.         0.62938596] \n",
            "\n",
            "\n",
            "I:20 Test-Acc:0.5354 Train-Acc:0.454\n",
            " [0.         0.69740784] \n",
            "\n",
            "\n",
            "I:30 Test-Acc:0.5918 Train-Acc:0.509\n",
            " [0.         0.74356075] \n",
            "\n",
            "\n",
            "I:40 Test-Acc:0.5925 Train-Acc:0.557\n",
            " [0.         0.74411303] \n",
            "\n",
            "\n",
            "I:50 Test-Acc:0.5839 Train-Acc:0.573\n",
            " [0.         0.73729402] \n",
            "\n",
            "\n",
            "I:60 Test-Acc:0.5821 Train-Acc:0.571\n",
            " [0.        0.7358574] \n",
            "\n",
            "\n",
            "I:70 Test-Acc:0.6016 Train-Acc:0.588\n",
            " [0.         0.75124875] \n",
            "\n",
            "\n",
            "I:80 Test-Acc:0.6344 Train-Acc:0.632\n",
            " [0.         0.77630935] \n",
            "\n",
            "\n",
            "I:90 Test-Acc:0.6748 Train-Acc:0.665\n",
            " [0.         0.80582756] \n",
            "\n",
            "\n",
            "I:100 Test-Acc:0.7186 Train-Acc:0.706\n",
            " [0.         0.83626207] \n",
            "\n",
            "\n",
            "I:110 Test-Acc:0.7465 Train-Acc:0.714\n",
            " [0.         0.85485256] \n",
            "\n",
            "\n",
            "I:120 Test-Acc:0.7661 Train-Acc:0.751\n",
            " [0.         0.86756129] \n",
            "\n",
            "\n",
            "I:130 Test-Acc:0.7825 Train-Acc:0.76\n",
            " [0.         0.87798036] \n",
            "\n",
            "\n",
            "I:140 Test-Acc:0.7948 Train-Acc:0.778\n",
            " [0.         0.88566971] \n",
            "\n",
            "\n",
            "I:150 Test-Acc:0.8031 Train-Acc:0.798\n",
            " [0.         0.89079918] \n",
            "\n",
            "\n",
            "I:160 Test-Acc:0.8085 Train-Acc:0.822\n",
            " [0.         0.89411114] \n",
            "\n",
            "\n",
            "I:170 Test-Acc:0.8142 Train-Acc:0.826\n",
            " [0.         0.89758571] \n",
            "\n",
            "\n",
            "I:180 Test-Acc:0.8201 Train-Acc:0.82\n",
            " [0.         0.90115928] \n",
            "\n",
            "\n",
            "I:190 Test-Acc:0.8247 Train-Acc:0.84\n",
            " [0.         0.90392941] \n",
            "\n",
            "\n",
            "I:200 Test-Acc:0.8265 Train-Acc:0.842\n",
            " [0.         0.90500958] \n",
            "\n",
            "\n",
            "I:210 Test-Acc:0.8291 Train-Acc:0.853\n",
            " [0.         0.90656607] \n",
            "\n",
            "\n",
            "I:220 Test-Acc:0.8296 Train-Acc:0.859\n",
            " [0.         0.90686489] \n",
            "\n",
            "\n",
            "I:230 Test-Acc:0.8338 Train-Acc:0.85\n",
            " [0.         0.90936852] \n",
            "\n",
            "\n",
            "I:240 Test-Acc:0.8355 Train-Acc:0.847\n",
            " [0.         0.91037864] \n",
            "\n",
            "\n",
            "I:250 Test-Acc:0.838 Train-Acc:0.874\n",
            " [0.         0.91186072] \n",
            "\n",
            "\n",
            "I:260 Test-Acc:0.8379 Train-Acc:0.865\n",
            " [0.         0.91180151] \n",
            "\n",
            "\n",
            "I:270 Test-Acc:0.8375 Train-Acc:0.872\n",
            " [0.         0.91156463] \n",
            "\n",
            "\n",
            "I:280 Test-Acc:0.8395 Train-Acc:0.886\n",
            " [0.         0.91274803] \n",
            "\n",
            "\n",
            "I:290 Test-Acc:0.8391 Train-Acc:0.874\n",
            " [0.         0.91251155] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0akTQLdSlmd"
      },
      "source": [
        "# Keras implementation (Part 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ352OYeRI4A",
        "outputId": "0670ba00-7ec3-4805-af28-92e5e8e9fd7c"
      },
      "source": [
        "# We haven't learned much on this so I had to look up this guide: https://keras.io/examples/vision/mnist_convnet/\n",
        "\n",
        "####Setup####\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "####Prepare the data####\n",
        "\n",
        "# Model / data parameters\n",
        "num_class = 10 # There are 10 classes, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0\n",
        "input_shape = (28, 28, 1) # Changing to 28 by 28, similar to what we did above\n",
        "\n",
        "# loading mnist data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scaling the training and testing data appropriately from 0 to 1\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Making shape of images 28 by 28\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# \n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices. Needed for keras model\n",
        "y_train = keras.utils.to_categorical(y_train, num_class)\n",
        "y_test = keras.utils.to_categorical(y_test, num_class)\n",
        "\n",
        "####Build the model#### \n",
        "# This is the model they created to get 99% accuracy. Crazy. Mine is similar to an extent. However if I copy this same process, my results are much lower.\n",
        "# possibliy there is an issue with my code :(\n",
        "mnist_model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape), # input layer\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), # relu layer\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)), # not sure why we need this. possibly why my code doesn't have as good accuracy \n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"), # another relu layer\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)), # \n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5), # dropout rate\n",
        "        layers.Dense(num_class, activation=\"softmax\"), # softmax for result\n",
        "    ]\n",
        ")\n",
        "\n",
        "mnist_model.summary()\n",
        "\n",
        "####Train the mnist_model####\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "mnist_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "mnist_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "####Evaluate the trained mnist_model####\n",
        "score = mnist_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "422/422 [==============================] - 37s 86ms/step - loss: 0.7769 - accuracy: 0.7580 - val_loss: 0.0839 - val_accuracy: 0.9770\n",
            "Epoch 2/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.1245 - accuracy: 0.9607 - val_loss: 0.0606 - val_accuracy: 0.9828\n",
            "Epoch 3/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0925 - accuracy: 0.9716 - val_loss: 0.0487 - val_accuracy: 0.9867\n",
            "Epoch 4/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0724 - accuracy: 0.9772 - val_loss: 0.0461 - val_accuracy: 0.9860\n",
            "Epoch 5/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0671 - accuracy: 0.9791 - val_loss: 0.0377 - val_accuracy: 0.9897\n",
            "Epoch 6/15\n",
            "422/422 [==============================] - 35s 84ms/step - loss: 0.0574 - accuracy: 0.9830 - val_loss: 0.0390 - val_accuracy: 0.9892\n",
            "Epoch 7/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0522 - accuracy: 0.9838 - val_loss: 0.0340 - val_accuracy: 0.9902\n",
            "Epoch 8/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0512 - accuracy: 0.9841 - val_loss: 0.0329 - val_accuracy: 0.9903\n",
            "Epoch 9/15\n",
            "422/422 [==============================] - 35s 84ms/step - loss: 0.0456 - accuracy: 0.9865 - val_loss: 0.0338 - val_accuracy: 0.9905\n",
            "Epoch 10/15\n",
            "422/422 [==============================] - 35s 83ms/step - loss: 0.0445 - accuracy: 0.9865 - val_loss: 0.0332 - val_accuracy: 0.9905\n",
            "Epoch 11/15\n",
            "422/422 [==============================] - 35s 84ms/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 0.0289 - val_accuracy: 0.9923\n",
            "Epoch 12/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0409 - accuracy: 0.9869 - val_loss: 0.0316 - val_accuracy: 0.9915\n",
            "Epoch 13/15\n",
            "422/422 [==============================] - 36s 84ms/step - loss: 0.0379 - accuracy: 0.9882 - val_loss: 0.0309 - val_accuracy: 0.9917\n",
            "Epoch 14/15\n",
            "422/422 [==============================] - 36s 84ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 0.0294 - val_accuracy: 0.9920\n",
            "Epoch 15/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0320 - accuracy: 0.9895 - val_loss: 0.0280 - val_accuracy: 0.9915\n",
            "Test loss: 0.024474680423736572\n",
            "Test accuracy: 0.9909999966621399\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}