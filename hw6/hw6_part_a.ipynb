{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw6_part_a",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "db0HyI10GSFy"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Rs_XlZ3xON"
      },
      "source": [
        "#Creating AutoGrad Framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5R5USnS3wLI"
      },
      "source": [
        "class SGD(object):\n",
        "    \n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "        \n",
        "    def step(self, zero=True):\n",
        "        \n",
        "        for p in self.parameters:\n",
        "            \n",
        "            p.data -= p.grad.data * self.alpha\n",
        "            \n",
        "            if(zero):\n",
        "                p.grad.data *= 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1xdfJxB03Tr"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "    \n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "            \n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "            \n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if(self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "                    \n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return Tensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2MM2-ZT1JFP",
        "outputId": "7a02b76f-4817-49c4-b83e-6af48371a5c1"
      },
      "source": [
        "import numpy\n",
        "np.random.seed(0)\n",
        "\n",
        "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
        "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
        "\n",
        "w = list()\n",
        "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
        "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
        "\n",
        "optim = SGD(parameters=w, alpha=0.1)\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "    # Predict\n",
        "    pred = data.mm(w[0]).mm(w[1])\n",
        "    \n",
        "    # Compare\n",
        "    loss = ((pred - target)*(pred - target)).sum(0)\n",
        "    \n",
        "    # Learn\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    optim.step()\n",
        "\n",
        "    print(loss)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.58128304]\n",
            "[0.48988149]\n",
            "[0.41375111]\n",
            "[0.34489412]\n",
            "[0.28210124]\n",
            "[0.2254484]\n",
            "[0.17538853]\n",
            "[0.1324231]\n",
            "[0.09682769]\n",
            "[0.06849361]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM41gh5-7S7Q"
      },
      "source": [
        "# MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNrFM0_y7wiv",
        "outputId": "d817813e-cc0c-4e77-dbdc-6cb6ef94812a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89x8PAXz74sp"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/My Drive/CMPE 258/Homework/HW6/digit-recognizer/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/CMPE 258/Homework/HW6/digit-recognizer/test.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "slTPp-sF9Bx-",
        "outputId": "3f5e8b7a-5e95-4f3a-d1c6-d94f075ec9f9"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>1x1</th>\n",
              "      <th>1x2</th>\n",
              "      <th>1x3</th>\n",
              "      <th>1x4</th>\n",
              "      <th>1x5</th>\n",
              "      <th>1x6</th>\n",
              "      <th>1x7</th>\n",
              "      <th>1x8</th>\n",
              "      <th>1x9</th>\n",
              "      <th>1x10</th>\n",
              "      <th>1x11</th>\n",
              "      <th>1x12</th>\n",
              "      <th>1x13</th>\n",
              "      <th>1x14</th>\n",
              "      <th>1x15</th>\n",
              "      <th>1x16</th>\n",
              "      <th>1x17</th>\n",
              "      <th>1x18</th>\n",
              "      <th>1x19</th>\n",
              "      <th>1x20</th>\n",
              "      <th>1x21</th>\n",
              "      <th>1x22</th>\n",
              "      <th>1x23</th>\n",
              "      <th>1x24</th>\n",
              "      <th>1x25</th>\n",
              "      <th>1x26</th>\n",
              "      <th>1x27</th>\n",
              "      <th>1x28</th>\n",
              "      <th>2x1</th>\n",
              "      <th>2x2</th>\n",
              "      <th>2x3</th>\n",
              "      <th>2x4</th>\n",
              "      <th>2x5</th>\n",
              "      <th>2x6</th>\n",
              "      <th>2x7</th>\n",
              "      <th>2x8</th>\n",
              "      <th>2x9</th>\n",
              "      <th>2x10</th>\n",
              "      <th>2x11</th>\n",
              "      <th>...</th>\n",
              "      <th>27x17</th>\n",
              "      <th>27x18</th>\n",
              "      <th>27x19</th>\n",
              "      <th>27x20</th>\n",
              "      <th>27x21</th>\n",
              "      <th>27x22</th>\n",
              "      <th>27x23</th>\n",
              "      <th>27x24</th>\n",
              "      <th>27x25</th>\n",
              "      <th>27x26</th>\n",
              "      <th>27x27</th>\n",
              "      <th>27x28</th>\n",
              "      <th>28x1</th>\n",
              "      <th>28x2</th>\n",
              "      <th>28x3</th>\n",
              "      <th>28x4</th>\n",
              "      <th>28x5</th>\n",
              "      <th>28x6</th>\n",
              "      <th>28x7</th>\n",
              "      <th>28x8</th>\n",
              "      <th>28x9</th>\n",
              "      <th>28x10</th>\n",
              "      <th>28x11</th>\n",
              "      <th>28x12</th>\n",
              "      <th>28x13</th>\n",
              "      <th>28x14</th>\n",
              "      <th>28x15</th>\n",
              "      <th>28x16</th>\n",
              "      <th>28x17</th>\n",
              "      <th>28x18</th>\n",
              "      <th>28x19</th>\n",
              "      <th>28x20</th>\n",
              "      <th>28x21</th>\n",
              "      <th>28x22</th>\n",
              "      <th>28x23</th>\n",
              "      <th>28x24</th>\n",
              "      <th>28x25</th>\n",
              "      <th>28x26</th>\n",
              "      <th>28x27</th>\n",
              "      <th>28x28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>147</td>\n",
              "      <td>252</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  1x1  1x2  1x3  1x4  1x5  ...  28x23  28x24  28x25  28x26  28x27  28x28\n",
              "0      5    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "1      0    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "2      4    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "3      1    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "4      9    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "JONdPdsnBmjM",
        "outputId": "4ea6b6cd-2b9d-4977-a77e-1c026e1777c7"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>1x1</th>\n",
              "      <th>1x2</th>\n",
              "      <th>1x3</th>\n",
              "      <th>1x4</th>\n",
              "      <th>1x5</th>\n",
              "      <th>1x6</th>\n",
              "      <th>1x7</th>\n",
              "      <th>1x8</th>\n",
              "      <th>1x9</th>\n",
              "      <th>1x10</th>\n",
              "      <th>1x11</th>\n",
              "      <th>1x12</th>\n",
              "      <th>1x13</th>\n",
              "      <th>1x14</th>\n",
              "      <th>1x15</th>\n",
              "      <th>1x16</th>\n",
              "      <th>1x17</th>\n",
              "      <th>1x18</th>\n",
              "      <th>1x19</th>\n",
              "      <th>1x20</th>\n",
              "      <th>1x21</th>\n",
              "      <th>1x22</th>\n",
              "      <th>1x23</th>\n",
              "      <th>1x24</th>\n",
              "      <th>1x25</th>\n",
              "      <th>1x26</th>\n",
              "      <th>1x27</th>\n",
              "      <th>1x28</th>\n",
              "      <th>2x1</th>\n",
              "      <th>2x2</th>\n",
              "      <th>2x3</th>\n",
              "      <th>2x4</th>\n",
              "      <th>2x5</th>\n",
              "      <th>2x6</th>\n",
              "      <th>2x7</th>\n",
              "      <th>2x8</th>\n",
              "      <th>2x9</th>\n",
              "      <th>2x10</th>\n",
              "      <th>2x11</th>\n",
              "      <th>...</th>\n",
              "      <th>27x17</th>\n",
              "      <th>27x18</th>\n",
              "      <th>27x19</th>\n",
              "      <th>27x20</th>\n",
              "      <th>27x21</th>\n",
              "      <th>27x22</th>\n",
              "      <th>27x23</th>\n",
              "      <th>27x24</th>\n",
              "      <th>27x25</th>\n",
              "      <th>27x26</th>\n",
              "      <th>27x27</th>\n",
              "      <th>27x28</th>\n",
              "      <th>28x1</th>\n",
              "      <th>28x2</th>\n",
              "      <th>28x3</th>\n",
              "      <th>28x4</th>\n",
              "      <th>28x5</th>\n",
              "      <th>28x6</th>\n",
              "      <th>28x7</th>\n",
              "      <th>28x8</th>\n",
              "      <th>28x9</th>\n",
              "      <th>28x10</th>\n",
              "      <th>28x11</th>\n",
              "      <th>28x12</th>\n",
              "      <th>28x13</th>\n",
              "      <th>28x14</th>\n",
              "      <th>28x15</th>\n",
              "      <th>28x16</th>\n",
              "      <th>28x17</th>\n",
              "      <th>28x18</th>\n",
              "      <th>28x19</th>\n",
              "      <th>28x20</th>\n",
              "      <th>28x21</th>\n",
              "      <th>28x22</th>\n",
              "      <th>28x23</th>\n",
              "      <th>28x24</th>\n",
              "      <th>28x25</th>\n",
              "      <th>28x26</th>\n",
              "      <th>28x27</th>\n",
              "      <th>28x28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  1x1  1x2  1x3  1x4  1x5  ...  28x23  28x24  28x25  28x26  28x27  28x28\n",
              "0      7    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "1      2    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "2      1    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "3      0    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "4      4    0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PurQ7uUO_88m"
      },
      "source": [
        "def visualize_N_elems_of_dataset(X, y, row, col, name):\n",
        "  print(\"Visualizing the \" + name + \" dataset.\")\n",
        "  fig = plt.figure(figsize=(15,10))\n",
        "\n",
        "  # Plot row * col images of the dataset\n",
        "  for image in range(1, col*row+1):\n",
        "    ax = fig.add_subplot(row, col, image)\n",
        "    ax.title.set_text(str(y[image]))\n",
        "    ax.imshow(X[image], cmap='Accent')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "Yq5meAyg-O3R",
        "outputId": "f43f64fa-bead-4fee-f36c-826683aba298"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range (2):\n",
        "    # The first column is the label\n",
        "    label = train.iloc[i][0]\n",
        "\n",
        "    # The rest of columns are pixels\n",
        "    pixels = train.iloc[i][1:]\n",
        "\n",
        "    # Make those columns into a array of 8-bits pixels\n",
        "    # This array will be of 1D with length 784\n",
        "    # The pixel intensity values are integers from 0 to 255\n",
        "    pixels = np.array(pixels, dtype='uint8')\n",
        "\n",
        "    # Reshape the array into 28 x 28 array (2-dimensional array)\n",
        "    pixels = pixels.reshape((28, 28))\n",
        "\n",
        "    # Plot\n",
        "    plt.title('Label is {label}'.format(label=label))\n",
        "    plt.imshow(pixels, cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztdrAYjxGIZc"
      },
      "source": [
        "labels = train['label'].to_numpy()\n",
        "\n",
        "new_labels = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd-tZ5MfHkUA",
        "outputId": "984e7c6a-19ce-4ae0-87c2-1b4e34eb8df3"
      },
      "source": [
        "for i in labels:\n",
        "  new_labels.append([i])\n",
        "np.array(new_labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5],\n",
              "       [0],\n",
              "       [4],\n",
              "       ...,\n",
              "       [5],\n",
              "       [6],\n",
              "       [8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMuuzpD3HAeF",
        "outputId": "1e5f6238-f6c0-4dfe-a9b9-21170bddd241"
      },
      "source": [
        "features = train.drop(['label'], axis=1).to_numpy()\n",
        "np.array(features)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8apnhAP9TMW2"
      },
      "source": [
        "# Use Autograd to Train a Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmzzki9OGEYq",
        "outputId": "8ec1b286-120b-42e2-ae8d-2c2c3c48255a"
      },
      "source": [
        "import numpy\n",
        "np.random.seed(0)\n",
        "\n",
        "data = Tensor(np.array(features), autograd=True)\n",
        "target = Tensor(np.array(new_labels), autograd=True)\n",
        "\n",
        "w = list()\n",
        "w.append(Tensor(np.random.rand(784,784), autograd=True))\n",
        "w.append(Tensor(np.random.rand(784,784), autograd=True))\n",
        "\n",
        "optim = SGD(parameters=w, alpha=0.1)\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "    # Predict\n",
        "    pred = data.mm(w[0]).mm(w[1])\n",
        "    \n",
        "    # Compare\n",
        "    loss = ((pred - target)*(pred - target)).sum(0)\n",
        "    \n",
        "    # Learn\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    optim.step()\n",
        "\n",
        "    print(loss)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.81913822e+18 1.86971041e+18 1.65153400e+18 1.63366916e+18\n",
            " 1.81497529e+18 1.81076473e+18 1.67066441e+18 1.73327613e+18\n",
            " 1.70172022e+18 1.66138157e+18 1.72434179e+18 1.69746370e+18\n",
            " 1.81382953e+18 1.77493395e+18 1.80977492e+18 1.71273722e+18\n",
            " 1.69020733e+18 1.85803081e+18 1.72642090e+18 1.77411729e+18\n",
            " 1.77581248e+18 1.60638568e+18 1.78135254e+18 1.70865125e+18\n",
            " 1.79395903e+18 1.78460022e+18 1.75724954e+18 1.79389641e+18\n",
            " 1.70521669e+18 1.80313614e+18 1.67420826e+18 1.78439952e+18\n",
            " 1.66837205e+18 1.68664810e+18 1.85125036e+18 1.72893243e+18\n",
            " 1.80168101e+18 1.68239291e+18 1.70789139e+18 1.82511081e+18\n",
            " 1.78255632e+18 1.70532662e+18 1.76937323e+18 1.73988480e+18\n",
            " 1.83021109e+18 1.91925031e+18 1.75438859e+18 1.75326086e+18\n",
            " 1.70734405e+18 1.77810330e+18 1.81485210e+18 1.76563927e+18\n",
            " 1.81374901e+18 1.61949485e+18 1.73065440e+18 1.73081971e+18\n",
            " 1.85996171e+18 1.75159292e+18 1.75737511e+18 1.62748283e+18\n",
            " 1.87626271e+18 1.81592573e+18 1.63942223e+18 1.72056311e+18\n",
            " 1.78246996e+18 1.82204993e+18 1.54876387e+18 1.73555605e+18\n",
            " 1.71796655e+18 1.64852963e+18 1.86858086e+18 1.66381012e+18\n",
            " 1.89668637e+18 1.72583067e+18 1.78394912e+18 1.69976474e+18\n",
            " 1.66583862e+18 1.59352376e+18 1.69602659e+18 1.76209878e+18\n",
            " 1.67223371e+18 1.63009001e+18 1.68709280e+18 1.76856502e+18\n",
            " 1.77530519e+18 1.73351264e+18 1.72517189e+18 1.81181656e+18\n",
            " 1.66418415e+18 1.81788073e+18 1.62594207e+18 1.72616346e+18\n",
            " 1.73151864e+18 1.61574245e+18 1.76557056e+18 1.75182342e+18\n",
            " 1.84510420e+18 1.82106227e+18 1.82489550e+18 1.75613641e+18\n",
            " 1.66947846e+18 1.76009551e+18 1.72056207e+18 1.62649931e+18\n",
            " 1.68776705e+18 1.84399558e+18 1.71506536e+18 1.71910159e+18\n",
            " 1.73836883e+18 1.82020540e+18 1.74537239e+18 1.70878731e+18\n",
            " 1.83069470e+18 1.68569092e+18 1.89977805e+18 1.76686083e+18\n",
            " 1.59151255e+18 1.70824716e+18 1.77439585e+18 1.69682702e+18\n",
            " 1.65066917e+18 1.71565484e+18 1.82603957e+18 1.80344268e+18\n",
            " 1.83652756e+18 1.63156376e+18 1.72358175e+18 1.73235543e+18\n",
            " 1.87644153e+18 1.77073429e+18 1.85154802e+18 1.73601412e+18\n",
            " 1.72218083e+18 1.72431444e+18 1.70848011e+18 1.77344653e+18\n",
            " 1.81237715e+18 1.78413339e+18 1.69857094e+18 1.69242980e+18\n",
            " 1.76768328e+18 1.80347571e+18 1.73768792e+18 1.78494244e+18\n",
            " 1.77939339e+18 1.63295457e+18 1.80940137e+18 1.79066521e+18\n",
            " 1.83340728e+18 1.57509243e+18 1.72705538e+18 1.80174909e+18\n",
            " 1.65028499e+18 1.87968856e+18 1.71660401e+18 1.69397928e+18\n",
            " 1.55849573e+18 1.77057018e+18 1.70246092e+18 1.74476238e+18\n",
            " 1.71903960e+18 1.71537613e+18 1.71602120e+18 1.68320378e+18\n",
            " 1.69977446e+18 1.77622980e+18 1.86691560e+18 1.86293528e+18\n",
            " 1.83383816e+18 1.65411927e+18 1.71438465e+18 1.65318474e+18\n",
            " 1.67729382e+18 1.85785644e+18 1.70554493e+18 1.78447916e+18\n",
            " 1.80709268e+18 1.73566926e+18 1.72502409e+18 1.86893479e+18\n",
            " 1.78445948e+18 1.73603201e+18 1.74929581e+18 1.79214986e+18\n",
            " 1.82377268e+18 1.76916965e+18 1.69960056e+18 1.82781948e+18\n",
            " 1.68139218e+18 1.86477360e+18 1.77907685e+18 1.76395150e+18\n",
            " 1.74821372e+18 1.75803171e+18 1.81264916e+18 1.69483089e+18\n",
            " 1.73914106e+18 1.72025847e+18 1.78130426e+18 1.87772767e+18\n",
            " 1.76465440e+18 1.80223696e+18 1.62034925e+18 1.87319819e+18\n",
            " 1.64626955e+18 1.80342897e+18 1.78107196e+18 1.72457063e+18\n",
            " 1.76940102e+18 1.74273654e+18 1.84025119e+18 1.72337782e+18\n",
            " 1.76829656e+18 1.75279143e+18 1.83507176e+18 1.76996416e+18\n",
            " 1.74029402e+18 1.73170202e+18 1.80055398e+18 1.78122341e+18\n",
            " 1.73452573e+18 1.71440047e+18 1.70078460e+18 1.72447877e+18\n",
            " 1.78078475e+18 1.70135785e+18 1.71361477e+18 1.88131291e+18\n",
            " 1.80072797e+18 1.81058717e+18 1.71807874e+18 1.83549061e+18\n",
            " 1.77662112e+18 1.89313124e+18 1.77985885e+18 1.67892401e+18\n",
            " 1.76549396e+18 1.79117639e+18 1.80473512e+18 1.81546870e+18\n",
            " 1.59946544e+18 1.80964828e+18 1.72166821e+18 1.72027211e+18\n",
            " 1.72688405e+18 1.78316340e+18 1.81382735e+18 1.74718429e+18\n",
            " 1.67620564e+18 1.73432047e+18 1.74783501e+18 1.74297577e+18\n",
            " 1.78039279e+18 1.76897488e+18 1.76608599e+18 1.59993361e+18\n",
            " 1.65748618e+18 1.73003843e+18 1.74353747e+18 1.65350450e+18\n",
            " 1.84470634e+18 1.78185564e+18 1.69691230e+18 1.80227287e+18\n",
            " 1.76159438e+18 1.72536461e+18 1.80490502e+18 1.85527814e+18\n",
            " 1.75556162e+18 1.53832068e+18 1.71961774e+18 1.62348457e+18\n",
            " 1.64949861e+18 1.70423376e+18 1.75048976e+18 1.64355025e+18\n",
            " 1.65886000e+18 1.70857052e+18 1.71298847e+18 1.80349670e+18\n",
            " 1.77257089e+18 1.77414546e+18 1.73175824e+18 1.63827980e+18\n",
            " 1.72742049e+18 1.51853399e+18 1.64928439e+18 1.79500089e+18\n",
            " 1.64419482e+18 1.73508264e+18 1.77529698e+18 1.84177106e+18\n",
            " 1.73981322e+18 1.72671321e+18 1.79531846e+18 1.65625953e+18\n",
            " 1.72249547e+18 1.71276248e+18 1.81188126e+18 1.80786570e+18\n",
            " 1.75307248e+18 1.76982909e+18 1.68720131e+18 1.83529769e+18\n",
            " 1.75588263e+18 1.69156607e+18 1.76019539e+18 1.77754622e+18\n",
            " 1.86594690e+18 1.71791970e+18 1.72458370e+18 1.67475065e+18\n",
            " 1.80825482e+18 1.71475534e+18 1.79827343e+18 1.71433055e+18\n",
            " 1.79724091e+18 1.78546934e+18 1.69039828e+18 1.71851176e+18\n",
            " 1.81980123e+18 1.77983901e+18 1.61664911e+18 1.62383028e+18\n",
            " 1.76026245e+18 1.65222697e+18 1.78496991e+18 1.74863608e+18\n",
            " 1.58221485e+18 1.80100717e+18 1.74253589e+18 1.70254736e+18\n",
            " 1.73766008e+18 1.67497247e+18 1.81836266e+18 1.78187482e+18\n",
            " 1.61286500e+18 1.75769822e+18 1.64843195e+18 1.65375501e+18\n",
            " 1.73834613e+18 1.78888035e+18 1.72139143e+18 1.82216280e+18\n",
            " 1.76651298e+18 1.82673505e+18 1.57942569e+18 1.72307074e+18\n",
            " 1.83928403e+18 1.89093497e+18 1.64050521e+18 1.74818624e+18\n",
            " 1.78743277e+18 1.59086713e+18 1.69654949e+18 1.71721197e+18\n",
            " 1.69011190e+18 1.63446082e+18 1.67947977e+18 1.80531929e+18\n",
            " 1.75071848e+18 1.79911611e+18 1.73594646e+18 1.65633772e+18\n",
            " 1.76140673e+18 1.87788103e+18 1.65580995e+18 1.76276445e+18\n",
            " 1.67754156e+18 1.87902685e+18 1.82889365e+18 1.66777793e+18\n",
            " 1.68192519e+18 1.72831962e+18 1.62201556e+18 1.76508430e+18\n",
            " 1.75457298e+18 1.85701042e+18 1.75634818e+18 1.65913679e+18\n",
            " 1.76949335e+18 1.77255079e+18 1.68365191e+18 1.75236889e+18\n",
            " 1.65741027e+18 1.76911990e+18 1.75325958e+18 1.76786417e+18\n",
            " 1.76357984e+18 1.66718308e+18 1.70516943e+18 1.57529485e+18\n",
            " 1.84991804e+18 1.66223311e+18 1.70419581e+18 1.70284016e+18\n",
            " 1.82919918e+18 1.74124148e+18 1.77925974e+18 1.72229144e+18\n",
            " 1.72614625e+18 1.71449813e+18 1.67576381e+18 1.79548913e+18\n",
            " 1.78944899e+18 1.82788896e+18 1.81141952e+18 1.66890201e+18\n",
            " 1.71527591e+18 1.68571003e+18 1.73400104e+18 1.71555791e+18\n",
            " 1.65924501e+18 1.68445894e+18 1.76979500e+18 1.82295352e+18\n",
            " 1.78518935e+18 1.71538988e+18 1.81144041e+18 1.70639884e+18\n",
            " 1.76777480e+18 1.72701605e+18 1.75652934e+18 1.84454424e+18\n",
            " 1.79040713e+18 1.77928127e+18 1.84008792e+18 1.71296969e+18\n",
            " 1.81927921e+18 1.75618159e+18 1.64826451e+18 1.65263568e+18\n",
            " 1.83882922e+18 1.66857661e+18 1.77107714e+18 1.71605474e+18\n",
            " 1.73054053e+18 1.85149709e+18 1.60027191e+18 1.77073577e+18\n",
            " 1.82616886e+18 1.85369521e+18 1.77438835e+18 1.66255518e+18\n",
            " 1.70027758e+18 1.68680155e+18 1.78756809e+18 1.75359448e+18\n",
            " 1.75647400e+18 1.67923284e+18 1.80188864e+18 1.73379075e+18\n",
            " 1.84025663e+18 1.69638238e+18 1.87031121e+18 1.74451199e+18\n",
            " 1.82924175e+18 1.72854511e+18 1.68760004e+18 1.77615158e+18\n",
            " 1.72692553e+18 1.80260161e+18 1.70664820e+18 1.83741204e+18\n",
            " 1.65574594e+18 1.59998818e+18 1.79414666e+18 1.71008489e+18\n",
            " 1.79583137e+18 1.67746170e+18 1.71274649e+18 1.67065953e+18\n",
            " 1.79302436e+18 1.70968029e+18 1.71406383e+18 1.80045149e+18\n",
            " 1.73400771e+18 1.74943975e+18 1.77911770e+18 1.84969412e+18\n",
            " 1.76642375e+18 1.69535782e+18 1.62403104e+18 1.74052663e+18\n",
            " 1.72505867e+18 1.76102441e+18 1.83547844e+18 1.82812414e+18\n",
            " 1.80878113e+18 1.76103918e+18 1.78901912e+18 1.71444396e+18\n",
            " 1.61068127e+18 1.80863246e+18 1.73963572e+18 1.74730065e+18\n",
            " 1.66904924e+18 1.73678294e+18 1.67902085e+18 1.73111372e+18\n",
            " 1.69652201e+18 1.74355339e+18 1.77786026e+18 1.75784643e+18\n",
            " 1.77612128e+18 1.79138128e+18 1.78483309e+18 1.81302278e+18\n",
            " 1.83919689e+18 1.68588622e+18 1.70854156e+18 1.66559498e+18\n",
            " 1.75511549e+18 1.71344612e+18 1.65866094e+18 1.81684560e+18\n",
            " 1.63438164e+18 1.78518486e+18 1.68248206e+18 1.64595002e+18\n",
            " 1.80411496e+18 1.83763367e+18 1.70003905e+18 1.78855861e+18\n",
            " 1.77885154e+18 1.74854789e+18 1.84214562e+18 1.77425250e+18\n",
            " 1.79181123e+18 1.69648022e+18 1.78581843e+18 1.85440694e+18\n",
            " 1.77157980e+18 1.70643016e+18 1.74561302e+18 1.65802474e+18\n",
            " 1.78156313e+18 1.81956977e+18 1.68653425e+18 1.68409372e+18\n",
            " 1.79214037e+18 1.87507597e+18 1.77127318e+18 1.72124222e+18\n",
            " 1.84778229e+18 1.70520875e+18 1.76108809e+18 1.60175289e+18\n",
            " 1.74264178e+18 1.84396717e+18 1.77510977e+18 1.86145980e+18\n",
            " 1.64329349e+18 1.73452592e+18 1.64781571e+18 1.83284120e+18\n",
            " 1.80306311e+18 1.68556571e+18 1.73397792e+18 1.80058355e+18\n",
            " 1.66370446e+18 1.74314166e+18 1.77927800e+18 1.76527880e+18\n",
            " 1.72054728e+18 1.69984095e+18 1.69346990e+18 1.65906879e+18\n",
            " 1.71355189e+18 1.81289636e+18 1.78725068e+18 1.85068253e+18\n",
            " 1.66347191e+18 1.76655611e+18 1.60494482e+18 1.83195708e+18\n",
            " 1.73984069e+18 1.67726162e+18 1.71421594e+18 1.73271652e+18\n",
            " 1.73944436e+18 1.78733782e+18 1.80478486e+18 1.73029651e+18\n",
            " 1.80532895e+18 1.83024106e+18 1.81345655e+18 1.85850611e+18\n",
            " 1.70426702e+18 1.65447722e+18 1.83726595e+18 1.81499472e+18\n",
            " 1.74535387e+18 1.68445193e+18 1.57786046e+18 1.77573299e+18\n",
            " 1.58841316e+18 1.79489266e+18 1.82879026e+18 1.85917284e+18\n",
            " 1.73545817e+18 1.66503931e+18 1.88949959e+18 1.79790843e+18\n",
            " 1.66874645e+18 1.76967199e+18 1.73891969e+18 1.72448861e+18\n",
            " 1.81701232e+18 1.75151090e+18 1.73830987e+18 1.83286303e+18\n",
            " 1.73362308e+18 1.82304194e+18 1.75069757e+18 1.71161577e+18\n",
            " 1.74329035e+18 1.83712821e+18 1.73956949e+18 1.81516539e+18\n",
            " 1.65217318e+18 1.82415471e+18 1.78168096e+18 1.85946483e+18\n",
            " 1.67778260e+18 1.65351668e+18 1.75207646e+18 1.71040795e+18\n",
            " 1.79155552e+18 1.76828870e+18 1.75321401e+18 1.70042360e+18\n",
            " 1.70436634e+18 1.79018177e+18 1.77452236e+18 1.85008430e+18\n",
            " 1.74850021e+18 1.79228514e+18 1.83639673e+18 1.77344632e+18\n",
            " 1.76054280e+18 1.65193897e+18 1.78856651e+18 1.70821285e+18\n",
            " 1.69824253e+18 1.77049454e+18 1.75436252e+18 1.85567233e+18\n",
            " 1.80357988e+18 1.76267569e+18 1.77334991e+18 1.73214442e+18\n",
            " 1.73277650e+18 1.73917965e+18 1.86228676e+18 1.70750169e+18\n",
            " 1.75282225e+18 1.67801141e+18 1.73173113e+18 1.68901250e+18\n",
            " 1.71682937e+18 1.69631027e+18 1.77084961e+18 1.71816804e+18\n",
            " 1.72835122e+18 1.84971278e+18 1.68380641e+18 1.69916436e+18\n",
            " 1.80457801e+18 1.82093127e+18 1.80244849e+18 1.79441801e+18\n",
            " 1.79019876e+18 1.96157729e+18 1.67813178e+18 1.72825926e+18\n",
            " 1.75302456e+18 1.86320349e+18 1.83562717e+18 1.64979976e+18\n",
            " 1.77440354e+18 1.77932305e+18 1.68971957e+18 1.76795458e+18\n",
            " 1.74892569e+18 1.87548149e+18 1.76431795e+18 1.79324862e+18\n",
            " 1.87053053e+18 1.78936304e+18 1.80962541e+18 1.87807397e+18\n",
            " 1.75618407e+18 1.66835814e+18 1.77448745e+18 1.75076656e+18\n",
            " 1.62757021e+18 1.76150739e+18 1.66721964e+18 1.78271041e+18\n",
            " 1.69190687e+18 1.68349415e+18 1.72872900e+18 1.60970290e+18\n",
            " 1.74443353e+18 1.68435818e+18 1.73985280e+18 1.65520618e+18\n",
            " 1.75115862e+18 1.73429691e+18 1.80752816e+18 1.71401054e+18\n",
            " 1.79219321e+18 1.93241901e+18 1.67880932e+18 1.75873470e+18\n",
            " 1.81094420e+18 1.74881434e+18 1.72443853e+18 1.63135509e+18\n",
            " 1.70326491e+18 1.81783546e+18 1.68374989e+18 1.78714734e+18\n",
            " 1.85593773e+18 1.83722786e+18 1.82148619e+18 1.84084342e+18\n",
            " 1.65797938e+18 1.72116422e+18 1.73319626e+18 1.78266721e+18\n",
            " 1.64163985e+18 1.80497172e+18 1.69990693e+18 1.78517434e+18\n",
            " 1.69295126e+18 1.72676561e+18 1.73803409e+18 1.72205110e+18\n",
            " 1.62522629e+18 1.77760474e+18 1.72608959e+18 1.76873658e+18\n",
            " 1.92154074e+18 1.67561902e+18 1.73213373e+18 1.65564033e+18\n",
            " 1.72785012e+18 1.68981944e+18 1.85326095e+18 1.75595622e+18\n",
            " 1.61519971e+18 1.71017231e+18 1.75807192e+18 1.75734319e+18\n",
            " 1.71946162e+18 1.69962851e+18 1.73745854e+18 1.72322216e+18\n",
            " 1.78400615e+18 1.71795332e+18 1.69441462e+18 1.65026686e+18\n",
            " 1.77919877e+18 1.69663469e+18 1.67525350e+18 1.67790230e+18\n",
            " 1.76262467e+18 1.78889365e+18 1.70032997e+18 1.74839613e+18\n",
            " 1.81609929e+18 1.63798885e+18 1.81247062e+18 1.66363944e+18\n",
            " 1.74725167e+18 1.66052531e+18 1.79399761e+18 1.79910034e+18\n",
            " 1.70769328e+18 1.65880574e+18 1.65276483e+18 1.67194716e+18]\n",
            "[1.18870086e+80 1.22174711e+80 1.07918146e+80 1.06750802e+80\n",
            " 1.18598111e+80 1.18322936e+80 1.09168184e+80 1.13259510e+80\n",
            " 1.11197523e+80 1.08561628e+80 1.12675739e+80 1.10919408e+80\n",
            " 1.18523166e+80 1.15981599e+80 1.18258239e+80 1.11917422e+80\n",
            " 1.10445241e+80 1.21411506e+80 1.12811566e+80 1.15928252e+80\n",
            " 1.16039021e+80 1.04967969e+80 1.16401023e+80 1.11650425e+80\n",
            " 1.17224808e+80 1.16613249e+80 1.14826043e+80 1.17220724e+80\n",
            " 1.11426021e+80 1.17824461e+80 1.09399781e+80 1.16600144e+80\n",
            " 1.09018398e+80 1.10212617e+80 1.20968414e+80 1.12975676e+80\n",
            " 1.17729368e+80 1.09934606e+80 1.11600786e+80 1.19260399e+80\n",
            " 1.16479696e+80 1.11433192e+80 1.15618249e+80 1.13691350e+80\n",
            " 1.19593665e+80 1.25411829e+80 1.14639110e+80 1.14565401e+80\n",
            " 1.11564989e+80 1.16188720e+80 1.18590048e+80 1.15374276e+80\n",
            " 1.18517940e+80 1.05824564e+80 1.13088215e+80 1.13099019e+80\n",
            " 1.21537687e+80 1.14456412e+80 1.14834261e+80 1.06346524e+80\n",
            " 1.22602844e+80 1.18660163e+80 1.07126676e+80 1.12428793e+80\n",
            " 1.16474034e+80 1.19060372e+80 1.01202703e+80 1.13408507e+80\n",
            " 1.12259115e+80 1.07721843e+80 1.22100903e+80 1.08720301e+80\n",
            " 1.23937430e+80 1.12772978e+80 1.16570721e+80 1.11069723e+80\n",
            " 1.08852869e+80 1.04127548e+80 1.10825470e+80 1.15142876e+80\n",
            " 1.09270734e+80 1.06516864e+80 1.10241720e+80 1.15565450e+80\n",
            " 1.16005862e+80 1.13274940e+80 1.12729938e+80 1.18391708e+80\n",
            " 1.08744725e+80 1.18787928e+80 1.06245869e+80 1.12794758e+80\n",
            " 1.13144685e+80 1.05579376e+80 1.15369777e+80 1.14471469e+80\n",
            " 1.20566801e+80 1.18995856e+80 1.19246332e+80 1.14753315e+80\n",
            " 1.09090713e+80 1.15012006e+80 1.12428729e+80 1.06282269e+80\n",
            " 1.10285755e+80 1.20494374e+80 1.12069523e+80 1.12333263e+80\n",
            " 1.13592326e+80 1.18939849e+80 1.14049960e+80 1.11659312e+80\n",
            " 1.19625269e+80 1.10150110e+80 1.24139468e+80 1.15454087e+80\n",
            " 1.03996072e+80 1.11624038e+80 1.15946436e+80 1.10877791e+80\n",
            " 1.07861631e+80 1.12108077e+80 1.19321085e+80 1.17844514e+80\n",
            " 1.20006412e+80 1.06613189e+80 1.12626038e+80 1.13199352e+80\n",
            " 1.22614567e+80 1.15707188e+80 1.20987905e+80 1.13438432e+80\n",
            " 1.12534526e+80 1.12673934e+80 1.11639222e+80 1.15884407e+80\n",
            " 1.18428316e+80 1.16582746e+80 1.10991765e+80 1.10590443e+80\n",
            " 1.15507821e+80 1.17846670e+80 1.13547811e+80 1.16635639e+80\n",
            " 1.16272976e+80 1.06704111e+80 1.18233843e+80 1.17009581e+80\n",
            " 1.19802498e+80 1.02923113e+80 1.12853005e+80 1.17733848e+80\n",
            " 1.07836562e+80 1.22826730e+80 1.12170100e+80 1.10691690e+80\n",
            " 1.01838618e+80 1.15696464e+80 1.11245932e+80 1.14010100e+80\n",
            " 1.12329238e+80 1.12089862e+80 1.12131995e+80 1.09987556e+80\n",
            " 1.11070354e+80 1.16066298e+80 1.21992075e+80 1.21732006e+80\n",
            " 1.19830644e+80 1.08087090e+80 1.12025100e+80 1.08026010e+80\n",
            " 1.09601384e+80 1.21400120e+80 1.11447452e+80 1.16605366e+80\n",
            " 1.18082977e+80 1.13415900e+80 1.12720316e+80 1.22124029e+80\n",
            " 1.16604021e+80 1.13439573e+80 1.14306290e+80 1.17106548e+80\n",
            " 1.19172937e+80 1.15604897e+80 1.11059007e+80 1.19437347e+80\n",
            " 1.09869212e+80 1.21852139e+80 1.16252260e+80 1.15263972e+80\n",
            " 1.14235569e+80 1.14877146e+80 1.18446085e+80 1.10747357e+80\n",
            " 1.13642738e+80 1.12408914e+80 1.16397856e+80 1.22698548e+80\n",
            " 1.15309887e+80 1.17765704e+80 1.05880379e+80 1.22402635e+80\n",
            " 1.07574134e+80 1.17843611e+80 1.16382708e+80 1.12690649e+80\n",
            " 1.15620075e+80 1.13877691e+80 1.20249731e+80 1.12612712e+80\n",
            " 1.15547907e+80 1.14534699e+80 1.19911278e+80 1.15656852e+80\n",
            " 1.13718113e+80 1.13156646e+80 1.17655717e+80 1.16392597e+80\n",
            " 1.13341195e+80 1.12026108e+80 1.11136411e+80 1.12684653e+80\n",
            " 1.16363908e+80 1.11173862e+80 1.11974766e+80 1.22932873e+80\n",
            " 1.17667059e+80 1.18311333e+80 1.12266458e+80 1.19938644e+80\n",
            " 1.16091849e+80 1.23705116e+80 1.16303451e+80 1.09707925e+80\n",
            " 1.15364777e+80 1.17042982e+80 1.17928950e+80 1.18630306e+80\n",
            " 1.04515751e+80 1.18250000e+80 1.12501001e+80 1.12409782e+80\n",
            " 1.12841841e+80 1.16519302e+80 1.18523107e+80 1.14168301e+80\n",
            " 1.09530310e+80 1.13327794e+80 1.14210832e+80 1.13893331e+80\n",
            " 1.16338332e+80 1.15592208e+80 1.15403440e+80 1.04546367e+80\n",
            " 1.08307071e+80 1.13047959e+80 1.13930051e+80 1.08046912e+80\n",
            " 1.20540838e+80 1.16433924e+80 1.10883350e+80 1.17768063e+80\n",
            " 1.15109960e+80 1.12742545e+80 1.17940076e+80 1.21231641e+80\n",
            " 1.14715733e+80 1.00520330e+80 1.12367015e+80 1.06085264e+80\n",
            " 1.07785114e+80 1.11361799e+80 1.14384340e+80 1.07396439e+80\n",
            " 1.08396857e+80 1.11645138e+80 1.11933840e+80 1.17848046e+80\n",
            " 1.15827212e+80 1.15930077e+80 1.13160345e+80 1.07052038e+80\n",
            " 1.12876894e+80 9.92273703e+79 1.07771132e+80 1.17292875e+80\n",
            " 1.07438579e+80 1.13377574e+80 1.16005342e+80 1.20349059e+80\n",
            " 1.13686669e+80 1.12830671e+80 1.17313643e+80 1.08226926e+80\n",
            " 1.12555075e+80 1.11919065e+80 1.18395915e+80 1.18133475e+80\n",
            " 1.14553108e+80 1.15648030e+80 1.10248809e+80 1.19926045e+80\n",
            " 1.14736722e+80 1.10534004e+80 1.15018530e+80 1.16152301e+80\n",
            " 1.21928767e+80 1.12256060e+80 1.12691524e+80 1.09435224e+80\n",
            " 1.18158942e+80 1.12049297e+80 1.17506718e+80 1.12021542e+80\n",
            " 1.17439244e+80 1.16670039e+80 1.10457704e+80 1.12294776e+80\n",
            " 1.18913396e+80 1.16302156e+80 1.05638581e+80 1.06107850e+80\n",
            " 1.15022889e+80 1.07963379e+80 1.16637380e+80 1.14263220e+80\n",
            " 1.03388550e+80 1.17685349e+80 1.13864585e+80 1.11251561e+80\n",
            " 1.13545978e+80 1.09449714e+80 1.18819405e+80 1.16435103e+80\n",
            " 1.05391344e+80 1.14855344e+80 1.07715439e+80 1.08063285e+80\n",
            " 1.13590825e+80 1.16892896e+80 1.12482926e+80 1.19067757e+80\n",
            " 1.15431349e+80 1.19366545e+80 1.03206296e+80 1.12592652e+80\n",
            " 1.20186535e+80 1.23561634e+80 1.07197452e+80 1.14233780e+80\n",
            " 1.16798369e+80 1.03953933e+80 1.10859632e+80 1.12209818e+80\n",
            " 1.10438997e+80 1.06802507e+80 1.09744180e+80 1.17967112e+80\n",
            " 1.14399285e+80 1.17561799e+80 1.13434022e+80 1.08232028e+80\n",
            " 1.15097704e+80 1.22708621e+80 1.08197578e+80 1.15186382e+80\n",
            " 1.09617591e+80 1.22783456e+80 1.19507589e+80 1.08979610e+80\n",
            " 1.09904028e+80 1.12935659e+80 1.05989292e+80 1.15338000e+80\n",
            " 1.14651132e+80 1.21344836e+80 1.14767090e+80 1.08414944e+80\n",
            " 1.15626119e+80 1.15825902e+80 1.10016876e+80 1.14507106e+80\n",
            " 1.08302124e+80 1.15601608e+80 1.14565345e+80 1.15519632e+80\n",
            " 1.15239700e+80 1.08940725e+80 1.11422893e+80 1.02936301e+80\n",
            " 1.20881406e+80 1.08617227e+80 1.11359278e+80 1.11270704e+80\n",
            " 1.19527537e+80 1.13779988e+80 1.16264279e+80 1.12541727e+80\n",
            " 1.12793636e+80 1.12032479e+80 1.09501367e+80 1.17324787e+80\n",
            " 1.16930076e+80 1.19441930e+80 1.18365733e+80 1.09053009e+80\n",
            " 1.12083245e+80 1.10151364e+80 1.13306910e+80 1.12101733e+80\n",
            " 1.08422032e+80 1.10069598e+80 1.15645824e+80 1.19119423e+80\n",
            " 1.16651767e+80 1.12090758e+80 1.18367077e+80 1.11503201e+80\n",
            " 1.15513817e+80 1.12850449e+80 1.14778987e+80 1.20530216e+80\n",
            " 1.16992714e+80 1.16265705e+80 1.20239052e+80 1.11932618e+80\n",
            " 1.18879332e+80 1.14756265e+80 1.07704504e+80 1.07990127e+80\n",
            " 1.20156832e+80 1.09031741e+80 1.15729610e+80 1.12134202e+80\n",
            " 1.13080762e+80 1.20984578e+80 1.04568430e+80 1.15707255e+80\n",
            " 1.19329509e+80 1.21128220e+80 1.15945975e+80 1.08638299e+80\n",
            " 1.11103258e+80 1.10222655e+80 1.16807191e+80 1.14587186e+80\n",
            " 1.14775362e+80 1.09728115e+80 1.17742937e+80 1.13293155e+80\n",
            " 1.20250095e+80 1.10848712e+80 1.22213972e+80 1.13993725e+80\n",
            " 1.19530317e+80 1.12950369e+80 1.10274847e+80 1.16061195e+80\n",
            " 1.12844559e+80 1.17789554e+80 1.11519559e+80 1.20064211e+80\n",
            " 1.08193371e+80 1.04549933e+80 1.17237062e+80 1.11744127e+80\n",
            " 1.17347088e+80 1.09612287e+80 1.11917975e+80 1.09167898e+80\n",
            " 1.17163714e+80 1.11717668e+80 1.12004073e+80 1.17649035e+80\n",
            " 1.13307311e+80 1.14315682e+80 1.16254987e+80 1.20866780e+80\n",
            " 1.15425521e+80 1.10781753e+80 1.06120999e+80 1.13733306e+80\n",
            " 1.12722531e+80 1.15072676e+80 1.19937856e+80 1.19457289e+80\n",
            " 1.18193318e+80 1.15073685e+80 1.16901994e+80 1.12028946e+80\n",
            " 1.05248644e+80 1.18183639e+80 1.13675084e+80 1.14175928e+80\n",
            " 1.09062676e+80 1.13488674e+80 1.09714251e+80 1.13118229e+80\n",
            " 1.10857865e+80 1.13931095e+80 1.16172853e+80 1.14865061e+80\n",
            " 1.16059189e+80 1.17056315e+80 1.16628448e+80 1.18470506e+80\n",
            " 1.20180827e+80 1.10162839e+80 1.11643254e+80 1.08836918e+80\n",
            " 1.14686584e+80 1.11963760e+80 1.08383830e+80 1.18720307e+80\n",
            " 1.06797336e+80 1.16651417e+80 1.09940431e+80 1.07553278e+80\n",
            " 1.17888421e+80 1.20078687e+80 1.11087681e+80 1.16871865e+80\n",
            " 1.16237606e+80 1.14257430e+80 1.20373526e+80 1.15937082e+80\n",
            " 1.17084426e+80 1.10855105e+80 1.16692868e+80 1.21174725e+80\n",
            " 1.15762457e+80 1.11505310e+80 1.14065671e+80 1.08342201e+80\n",
            " 1.16414783e+80 1.18898300e+80 1.10205201e+80 1.10045700e+80\n",
            " 1.17105966e+80 1.22525334e+80 1.15742426e+80 1.12473149e+80\n",
            " 1.20741823e+80 1.11425490e+80 1.15076869e+80 1.04665215e+80\n",
            " 1.13871524e+80 1.20492529e+80 1.15993132e+80 1.21635564e+80\n",
            " 1.07379657e+80 1.13341180e+80 1.07675147e+80 1.19765526e+80\n",
            " 1.17819709e+80 1.10141916e+80 1.13305320e+80 1.17657680e+80\n",
            " 1.08713423e+80 1.13904149e+80 1.16265494e+80 1.15350688e+80\n",
            " 1.12427767e+80 1.11074705e+80 1.10658427e+80 1.08410504e+80\n",
            " 1.11970630e+80 1.18462255e+80 1.16786431e+80 1.20931363e+80\n",
            " 1.08698211e+80 1.15434185e+80 1.04873797e+80 1.19707773e+80\n",
            " 1.13688431e+80 1.09599295e+80 1.12014039e+80 1.13222969e+80\n",
            " 1.13662582e+80 1.16792127e+80 1.17932195e+80 1.13064834e+80\n",
            " 1.17967773e+80 1.19595602e+80 1.18498846e+80 1.21442572e+80\n",
            " 1.11363950e+80 1.08110476e+80 1.20054636e+80 1.18599359e+80\n",
            " 1.14048751e+80 1.10069136e+80 1.03104022e+80 1.16033844e+80\n",
            " 1.03793546e+80 1.17285788e+80 1.19500803e+80 1.21486093e+80\n",
            " 1.13402081e+80 1.08800649e+80 1.23467765e+80 1.17482854e+80\n",
            " 1.09042843e+80 1.15637805e+80 1.13628298e+80 1.12685292e+80\n",
            " 1.18731204e+80 1.14451058e+80 1.13588452e+80 1.19766954e+80\n",
            " 1.13282160e+80 1.19125183e+80 1.14397888e+80 1.11844158e+80\n",
            " 1.13913912e+80 1.20045655e+80 1.13670765e+80 1.18610520e+80\n",
            " 1.07959895e+80 1.19197931e+80 1.16422481e+80 1.21505226e+80\n",
            " 1.09633337e+80 1.08047663e+80 1.14488027e+80 1.11765180e+80\n",
            " 1.17067746e+80 1.15547402e+80 1.14562335e+80 1.11112801e+80\n",
            " 1.11370458e+80 1.16977968e+80 1.15954738e+80 1.20892265e+80\n",
            " 1.14254343e+80 1.17115417e+80 1.19997863e+80 1.15884419e+80\n",
            " 1.15041245e+80 1.07944623e+80 1.16872410e+80 1.11621765e+80\n",
            " 1.10970280e+80 1.15691547e+80 1.14637394e+80 1.21257402e+80\n",
            " 1.17853459e+80 1.15180599e+80 1.15878125e+80 1.13185561e+80\n",
            " 1.13226895e+80 1.13645310e+80 1.21689627e+80 1.11575326e+80\n",
            " 1.14536754e+80 1.09648307e+80 1.13158587e+80 1.10367163e+80\n",
            " 1.12184824e+80 1.10844001e+80 1.15714703e+80 1.12272274e+80\n",
            " 1.12937724e+80 1.20867951e+80 1.10026980e+80 1.11030494e+80\n",
            " 1.17918667e+80 1.18987257e+80 1.17779544e+80 1.17254802e+80\n",
            " 1.16979066e+80 1.28177678e+80 1.09656169e+80 1.12931704e+80\n",
            " 1.14549970e+80 1.21749534e+80 1.19947576e+80 1.07804796e+80\n",
            " 1.15946931e+80 1.16268419e+80 1.10413308e+80 1.15525564e+80\n",
            " 1.14282135e+80 1.22551820e+80 1.15287943e+80 1.17178369e+80\n",
            " 1.22228281e+80 1.16924479e+80 1.18248433e+80 1.22721222e+80\n",
            " 1.14756429e+80 1.09017505e+80 1.15952457e+80 1.14402429e+80\n",
            " 1.06352244e+80 1.15104276e+80 1.08943110e+80 1.16489777e+80\n",
            " 1.10556269e+80 1.10006562e+80 1.12962372e+80 1.05184692e+80\n",
            " 1.13988592e+80 1.10062983e+80 1.13689234e+80 1.08158116e+80\n",
            " 1.14428054e+80 1.13326207e+80 1.18111433e+80 1.12000600e+80\n",
            " 1.17109427e+80 1.26272371e+80 1.09700425e+80 1.14923112e+80\n",
            " 1.18334646e+80 1.14274827e+80 1.12682006e+80 1.06599584e+80\n",
            " 1.11298467e+80 1.18784975e+80 1.10023273e+80 1.16779708e+80\n",
            " 1.21274747e+80 1.20052146e+80 1.19023539e+80 1.20288427e+80\n",
            " 1.08339261e+80 1.12468096e+80 1.13254316e+80 1.16486905e+80\n",
            " 1.07271628e+80 1.17944410e+80 1.11079057e+80 1.16650773e+80\n",
            " 1.10624547e+80 1.12834121e+80 1.13570443e+80 1.12526031e+80\n",
            " 1.06199086e+80 1.16156113e+80 1.12789926e+80 1.15576668e+80\n",
            " 1.25561498e+80 1.09491979e+80 1.13184887e+80 1.08186431e+80\n",
            " 1.12904971e+80 1.10419844e+80 1.21099809e+80 1.14741529e+80\n",
            " 1.05543930e+80 1.11749825e+80 1.14879764e+80 1.14832167e+80\n",
            " 1.12356796e+80 1.11060836e+80 1.13532819e+80 1.12602552e+80\n",
            " 1.16574447e+80 1.12258271e+80 1.10720151e+80 1.07835372e+80\n",
            " 1.16260298e+80 1.10865201e+80 1.09468032e+80 1.09641100e+80\n",
            " 1.15177280e+80 1.16893807e+80 1.11106680e+80 1.14247516e+80\n",
            " 1.18671527e+80 1.07033053e+80 1.18434428e+80 1.08709168e+80\n",
            " 1.14172750e+80 1.08505659e+80 1.17227329e+80 1.17560740e+80\n",
            " 1.11587843e+80 1.08393307e+80 1.07998543e+80 1.09251997e+80]\n",
            "[3.35978455e+265 3.45318759e+265 3.05023519e+265 3.01724098e+265\n",
            " 3.35209737e+265 3.34431972e+265 3.08556668e+265 3.20120533e+265\n",
            " 3.14292461e+265 3.06842278e+265 3.18470541e+265 3.13506389e+265\n",
            " 3.34997908e+265 3.27814338e+265 3.34249110e+265 3.16327208e+265\n",
            " 3.12166187e+265 3.43161612e+265 3.18854447e+265 3.27663557e+265\n",
            " 3.27976637e+265 2.96685038e+265 3.28999812e+265 3.15572560e+265\n",
            " 3.31328186e+265 3.29599655e+265 3.24548233e+265 3.31316643e+265\n",
            " 3.14938297e+265 3.33023066e+265 3.09211264e+265 3.29562615e+265\n",
            " 3.08133310e+265 3.11508690e+265 3.41909240e+265 3.19318293e+265\n",
            " 3.32754292e+265 3.10722910e+265 3.15432258e+265 3.37081649e+265\n",
            " 3.29222177e+265 3.14958566e+265 3.26787352e+265 3.21341100e+265\n",
            " 3.38023603e+265 3.54468261e+265 3.24019881e+265 3.23811548e+265\n",
            " 3.15331079e+265 3.28399751e+265 3.35186947e+265 3.26097779e+265\n",
            " 3.34983139e+265 2.99106148e+265 3.19636377e+265 3.19666916e+265\n",
            " 3.43518254e+265 3.23503496e+265 3.24571462e+265 3.00581434e+265\n",
            " 3.46528851e+265 3.35385120e+265 3.02786482e+265 3.17772565e+265\n",
            " 3.29206173e+265 3.36516286e+265 2.86042764e+265 3.20541661e+265\n",
            " 3.17292981e+265 3.04468683e+265 3.45110147e+265 3.07290757e+265\n",
            " 3.50300968e+265 3.18745380e+265 3.29479451e+265 3.13931245e+265\n",
            " 3.07665453e+265 2.94309643e+265 3.13240880e+265 3.25443742e+265\n",
            " 3.08846519e+265 3.01062888e+265 3.11590949e+265 3.26638120e+265\n",
            " 3.27882914e+265 3.20164143e+265 3.18623732e+265 3.34626352e+265\n",
            " 3.07359789e+265 3.35746241e+265 3.00296940e+265 3.18806941e+265\n",
            " 3.19795986e+265 2.98413140e+265 3.26085064e+265 3.23546055e+265\n",
            " 3.40774109e+265 3.36333937e+265 3.37041890e+265 3.24342673e+265\n",
            " 3.08337703e+265 3.25073845e+265 3.17772385e+265 3.00399820e+265\n",
            " 3.11715410e+265 3.40569398e+265 3.16757111e+265 3.17502556e+265\n",
            " 3.21061214e+265 3.36175637e+265 3.22354687e+265 3.15597679e+265\n",
            " 3.38112930e+265 3.11332019e+265 3.50872017e+265 3.26323360e+265\n",
            " 2.93938036e+265 3.15497980e+265 3.27714952e+265 3.13388760e+265\n",
            " 3.04863782e+265 3.16866082e+265 3.37253174e+265 3.33079743e+265\n",
            " 3.39190205e+265 3.01335144e+265 3.18330066e+265 3.19950499e+265\n",
            " 3.46561984e+265 3.27038734e+265 3.41964330e+265 3.20626243e+265\n",
            " 3.18071412e+265 3.18465441e+265 3.15540896e+265 3.27539630e+265\n",
            " 3.34729820e+265 3.29513438e+265 3.13710900e+265 3.12576592e+265\n",
            " 3.26475234e+265 3.33085836e+265 3.20935396e+265 3.29662939e+265\n",
            " 3.28637894e+265 3.01592128e+265 3.34180156e+265 3.30719861e+265\n",
            " 3.38613856e+265 2.90905389e+265 3.18971574e+265 3.32766953e+265\n",
            " 3.04792926e+265 3.47161647e+265 3.17041387e+265 3.12862757e+265\n",
            " 2.87840134e+265 3.27008422e+265 3.14429286e+265 3.22242026e+265\n",
            " 3.17491181e+265 3.16814600e+265 3.16933685e+265 3.10872569e+265\n",
            " 3.13933026e+265 3.28053733e+265 3.44802551e+265 3.44067483e+265\n",
            " 3.38693408e+265 3.05501028e+265 3.16631555e+265 3.05328391e+265\n",
            " 3.09781082e+265 3.43129428e+265 3.14998871e+265 3.29577373e+265\n",
            " 3.33753743e+265 3.20562558e+265 3.18596536e+265 3.45175509e+265\n",
            " 3.29573572e+265 3.20629467e+265 3.23079186e+265 3.30993930e+265\n",
            " 3.36834443e+265 3.26749614e+265 3.13900957e+265 3.37581782e+265\n",
            " 3.10538079e+265 3.44407031e+265 3.28579342e+265 3.25786011e+265\n",
            " 3.22879298e+265 3.24692672e+265 3.34780045e+265 3.13020096e+265\n",
            " 3.21203702e+265 3.17716378e+265 3.28990860e+265 3.46799351e+265\n",
            " 3.25915788e+265 3.32856993e+265 2.99263905e+265 3.45962972e+265\n",
            " 3.04051193e+265 3.33077193e+265 3.28948046e+265 3.18512684e+265\n",
            " 3.26792512e+265 3.21867781e+265 3.39877932e+265 3.18292401e+265\n",
            " 3.26588536e+265 3.23724769e+265 3.38921315e+265 3.26896462e+265\n",
            " 3.21416743e+265 3.19829795e+265 3.32546122e+265 3.28975996e+265\n",
            " 3.20351410e+265 3.16634403e+265 3.14119732e+265 3.18495736e+265\n",
            " 3.28894909e+265 3.14225585e+265 3.16489288e+265 3.47461655e+265\n",
            " 3.32578180e+265 3.34399178e+265 3.17313736e+265 3.38998663e+265\n",
            " 3.28125951e+265 3.49644348e+265 3.28724032e+265 3.10082211e+265\n",
            " 3.26070931e+265 3.30814266e+265 3.33318395e+265 3.35300732e+265\n",
            " 2.95406874e+265 3.34225822e+265 3.17976656e+265 3.17718833e+265\n",
            " 3.18940019e+265 3.29334119e+265 3.34997741e+265 3.22689170e+265\n",
            " 3.09580194e+265 3.20313533e+265 3.22809382e+265 3.21911985e+265\n",
            " 3.28822620e+265 3.26713750e+265 3.26180209e+265 2.95493409e+265\n",
            " 3.06122789e+265 3.19522597e+265 3.22015773e+265 3.05387469e+265\n",
            " 3.40700727e+265 3.29092804e+265 3.13404473e+265 3.32863660e+265\n",
            " 3.25350706e+265 3.18659365e+265 3.33349844e+265 3.42653234e+265\n",
            " 3.24236449e+265 2.84114081e+265 3.17597954e+265 2.99843000e+265\n",
            " 3.04647514e+265 3.14756776e+265 3.23299789e+265 3.03548949e+265\n",
            " 3.06376563e+265 3.15557617e+265 3.16373612e+265 3.33089727e+265\n",
            " 3.27377973e+265 3.27668713e+265 3.19840249e+265 3.02575523e+265\n",
            " 3.19039094e+265 2.80459615e+265 3.04607995e+265 3.31520572e+265\n",
            " 3.03668056e+265 3.20454232e+265 3.27881447e+265 3.40158674e+265\n",
            " 3.21327868e+265 3.18908448e+265 3.31579273e+265 3.05896265e+265\n",
            " 3.18129494e+265 3.16331852e+265 3.34638243e+265 3.33896474e+265\n",
            " 3.23776803e+265 3.26871527e+265 3.11610986e+265 3.38963053e+265\n",
            " 3.24295775e+265 3.12417069e+265 3.25092286e+265 3.28296815e+265\n",
            " 3.44623615e+265 3.17284346e+265 3.18515158e+265 3.09311440e+265\n",
            " 3.33968453e+265 3.16699945e+265 3.32124987e+265 3.16621498e+265\n",
            " 3.31934276e+265 3.29760167e+265 3.12201413e+265 3.17393774e+265\n",
            " 3.36100868e+265 3.28720370e+265 2.98580480e+265 2.99906838e+265\n",
            " 3.25104607e+265 3.05151369e+265 3.29667858e+265 3.22957453e+265\n",
            " 2.92220915e+265 3.32629873e+265 3.21830736e+265 3.14445198e+265\n",
            " 3.20930216e+265 3.09352395e+265 3.35835208e+265 3.29096137e+265\n",
            " 2.97881680e+265 3.24631052e+265 3.04450580e+265 3.05433744e+265\n",
            " 3.21056971e+265 3.30390059e+265 3.17925569e+265 3.36537159e+265\n",
            " 3.26259092e+265 3.37381662e+265 2.91705787e+265 3.18235702e+265\n",
            " 3.39699312e+265 3.49238807e+265 3.02986524e+265 3.22874241e+265\n",
            " 3.30122883e+265 2.93818934e+265 3.13337436e+265 3.17153648e+265\n",
            " 3.12148539e+265 3.01870238e+265 3.10184684e+265 3.33426260e+265\n",
            " 3.23342032e+265 3.32280668e+265 3.20613778e+265 3.05910687e+265\n",
            " 3.25316067e+265 3.46827820e+265 3.05813317e+265 3.25566709e+265\n",
            " 3.09826888e+265 3.47039336e+265 3.37780315e+265 3.08023676e+265\n",
            " 3.10636483e+265 3.19205190e+265 2.99571742e+265 3.25995246e+265\n",
            " 3.24053861e+265 3.42973173e+265 3.24381608e+265 3.06427686e+265\n",
            " 3.26809598e+265 3.27374270e+265 3.10955441e+265 3.23646779e+265\n",
            " 3.06108809e+265 3.26740318e+265 3.23811389e+265 3.26508618e+265\n",
            " 3.25717408e+265 3.07913770e+265 3.14929455e+265 2.90942663e+265\n",
            " 3.41663319e+265 3.06999426e+265 3.14749653e+265 3.14499304e+265\n",
            " 3.37836696e+265 3.21591630e+265 3.28613314e+265 3.18091766e+265\n",
            " 3.18803770e+265 3.16652409e+265 3.09498390e+265 3.31610769e+265\n",
            " 3.30495145e+265 3.37594734e+265 3.34552934e+265 3.08231134e+265\n",
            " 3.16795896e+265 3.11335562e+265 3.20254505e+265 3.16848151e+265\n",
            " 3.06447721e+265 3.11104457e+265 3.26865290e+265 3.36683188e+265\n",
            " 3.29708522e+265 3.16817132e+265 3.34556732e+265 3.15156440e+265\n",
            " 3.26492183e+265 3.18964349e+265 3.24415234e+265 3.40670704e+265\n",
            " 3.30672187e+265 3.28617343e+265 3.39847747e+265 3.16370159e+265\n",
            " 3.36004589e+265 3.24351012e+265 3.04419676e+265 3.05226969e+265\n",
            " 3.39615358e+265 3.08171023e+265 3.27102107e+265 3.16939923e+265\n",
            " 3.19615312e+265 3.41954928e+265 2.95555768e+265 3.27038921e+265\n",
            " 3.37276983e+265 3.42360921e+265 3.27713649e+265 3.07058985e+265\n",
            " 3.14026030e+265 3.11537061e+265 3.30147817e+265 3.23873120e+265\n",
            " 3.24404988e+265 3.10139276e+265 3.32792644e+265 3.20215628e+265\n",
            " 3.39878961e+265 3.13306571e+265 3.45429730e+265 3.22195741e+265\n",
            " 3.37844555e+265 3.19246764e+265 3.11684578e+265 3.28039310e+265\n",
            " 3.18947700e+265 3.32924404e+265 3.15202675e+265 3.39353570e+265\n",
            " 3.05801425e+265 2.95503487e+265 3.31362822e+265 3.15837404e+265\n",
            " 3.31673802e+265 3.09811897e+265 3.16328772e+265 3.08555860e+265\n",
            " 3.31155509e+265 3.15762618e+265 3.16572122e+265 3.32527234e+265\n",
            " 3.20255638e+265 3.23105734e+265 3.28587052e+265 3.41621979e+265\n",
            " 3.26242621e+265 3.13117317e+265 2.99944002e+265 3.21459684e+265\n",
            " 3.18602797e+265 3.25245327e+265 3.38996436e+265 3.37638145e+265\n",
            " 3.34065614e+265 3.25248178e+265 3.30415774e+265 3.16642424e+265\n",
            " 2.97478347e+265 3.34038259e+265 3.21295125e+265 3.22710727e+265\n",
            " 3.08258458e+265 3.20768249e+265 3.10100092e+265 3.19721211e+265\n",
            " 3.13332443e+265 3.22018722e+265 3.28354905e+265 3.24658515e+265\n",
            " 3.28033642e+265 3.30851952e+265 3.29642612e+265 3.34849070e+265\n",
            " 3.39683178e+265 3.11367995e+265 3.15552291e+265 3.07620367e+265\n",
            " 3.24154064e+265 3.16458181e+265 3.06339745e+265 3.35555114e+265\n",
            " 3.01855623e+265 3.29707534e+265 3.10739375e+265 3.03992243e+265\n",
            " 3.33203843e+265 3.39394486e+265 3.13982002e+265 3.30330617e+265\n",
            " 3.28537924e+265 3.22941088e+265 3.40227830e+265 3.27688514e+265\n",
            " 3.30931406e+265 3.13324640e+265 3.29824692e+265 3.42492366e+265\n",
            " 3.27194947e+265 3.15162401e+265 3.22399093e+265 3.06222084e+265\n",
            " 3.29038702e+265 3.36058201e+265 3.11487728e+265 3.11036910e+265\n",
            " 3.30992287e+265 3.46309772e+265 3.27138331e+265 3.17897936e+265\n",
            " 3.41268798e+265 3.14936797e+265 3.25257177e+265 2.95829325e+265\n",
            " 3.21850350e+265 3.40564184e+265 3.27846935e+265 3.43794897e+265\n",
            " 3.03501517e+265 3.20351367e+265 3.04336698e+265 3.38509357e+265\n",
            " 3.33009633e+265 3.11308858e+265 3.20250011e+265 3.32551669e+265\n",
            " 3.07271318e+265 3.21942562e+265 3.28616747e+265 3.26031108e+265\n",
            " 3.17769665e+265 3.13945325e+265 3.12768744e+265 3.06415138e+265\n",
            " 3.16477598e+265 3.34825747e+265 3.30089142e+265 3.41804517e+265\n",
            " 3.07228322e+265 3.26267107e+265 2.96418868e+265 3.38346124e+265\n",
            " 3.21332850e+265 3.09775176e+265 3.16600290e+265 3.20017252e+265\n",
            " 3.21259790e+265 3.30105241e+265 3.33327568e+265 3.19570293e+265\n",
            " 3.33428126e+265 3.38029079e+265 3.34929169e+265 3.43249416e+265\n",
            " 3.14762856e+265 3.05567128e+265 3.39326507e+265 3.35213264e+265\n",
            " 3.22351270e+265 3.11103152e+265 2.91416714e+265 3.27962004e+265\n",
            " 2.93365609e+265 3.31500540e+265 3.37761136e+265 3.43372428e+265\n",
            " 3.20523501e+265 3.07517855e+265 3.48973492e+265 3.32057536e+265\n",
            " 3.08202401e+265 3.26842626e+265 3.21162886e+265 3.18497542e+265\n",
            " 3.35585914e+265 3.23488365e+265 3.21050266e+265 3.38513393e+265\n",
            " 3.20184552e+265 3.36699470e+265 3.23338082e+265 3.16120133e+265\n",
            " 3.21970156e+265 3.39301123e+265 3.21282916e+265 3.35244808e+265\n",
            " 3.05141520e+265 3.36905089e+265 3.29060462e+265 3.43426506e+265\n",
            " 3.09871394e+265 3.05389592e+265 3.23592854e+265 3.15896907e+265\n",
            " 3.30884260e+265 3.26587108e+265 3.23802882e+265 3.14053001e+265\n",
            " 3.14781251e+265 3.30630509e+265 3.27738416e+265 3.41694011e+265\n",
            " 3.22932363e+265 3.31019000e+265 3.39166043e+265 3.27539665e+265\n",
            " 3.25156488e+265 3.05098356e+265 3.30332155e+265 3.15491553e+265\n",
            " 3.13650175e+265 3.26994524e+265 3.24015031e+265 3.42726046e+265\n",
            " 3.33105027e+265 3.25550364e+265 3.27521876e+265 3.19911519e+265\n",
            " 3.20028349e+265 3.21210969e+265 3.43947702e+265 3.15360298e+265\n",
            " 3.23730579e+265 3.09913704e+265 3.19835281e+265 3.11945505e+265\n",
            " 3.17083004e+265 3.13293256e+265 3.27059973e+265 3.17330176e+265\n",
            " 3.19211025e+265 3.41625289e+265 3.10984001e+265 3.13820366e+265\n",
            " 3.33289333e+265 3.36309632e+265 3.32896110e+265 3.31412963e+265\n",
            " 3.30633611e+265 3.62285749e+265 3.09935926e+265 3.19194011e+265\n",
            " 3.23767932e+265 3.44117024e+265 3.39023908e+265 3.04703143e+265\n",
            " 3.27716351e+265 3.28625016e+265 3.12075931e+265 3.26525385e+265\n",
            " 3.23010914e+265 3.46384633e+265 3.25853764e+265 3.31196928e+265\n",
            " 3.45470171e+265 3.30479325e+265 3.34221395e+265 3.46863437e+265\n",
            " 3.24351474e+265 3.08130785e+265 3.27731970e+265 3.23350916e+265\n",
            " 3.00597601e+265 3.25334641e+265 3.07920512e+265 3.29250668e+265\n",
            " 3.12479999e+265 3.10926289e+265 3.19280691e+265 2.97297592e+265\n",
            " 3.22181235e+265 3.11085759e+265 3.21335120e+265 3.05701778e+265\n",
            " 3.23423344e+265 3.20309046e+265 3.33834171e+265 3.16562306e+265\n",
            " 3.31002069e+265 3.56900526e+265 3.10061013e+265 3.24822593e+265\n",
            " 3.34465070e+265 3.22990259e+265 3.18488255e+265 3.01296692e+265\n",
            " 3.14577773e+265 3.35737894e+265 3.10973522e+265 3.30070139e+265\n",
            " 3.42775072e+265 3.39319469e+265 3.36412180e+265 3.39987302e+265\n",
            " 3.06213774e+265 3.17883653e+265 3.20105851e+265 3.29242552e+265\n",
            " 3.03196178e+265 3.33362093e+265 3.13957627e+265 3.29705714e+265\n",
            " 3.12672982e+265 3.18918198e+265 3.20999364e+265 3.18047403e+265\n",
            " 3.00164711e+265 3.28307589e+265 3.18793284e+265 3.26669827e+265\n",
            " 3.54891290e+265 3.09471855e+265 3.19909616e+265 3.05781810e+265\n",
            " 3.19118452e+265 3.12094405e+265 3.42280620e+265 3.24309361e+265\n",
            " 2.98312956e+265 3.15853507e+265 3.24700074e+265 3.24565543e+265\n",
            " 3.17569071e+265 3.13906125e+265 3.20893022e+265 3.18263683e+265\n",
            " 3.29489982e+265 3.17290597e+265 3.12943200e+265 3.04789565e+265\n",
            " 3.28602061e+265 3.13353176e+265 3.09404171e+265 3.09893336e+265\n",
            " 3.25540981e+265 3.30392633e+265 3.14035699e+265 3.22913067e+265\n",
            " 3.35417242e+265 3.02521863e+265 3.34747096e+265 3.07259291e+265\n",
            " 3.22701746e+265 3.06684085e+265 3.31335310e+265 3.32277676e+265\n",
            " 3.15395675e+265 3.06366532e+265 3.05250756e+265 3.08793561e+265]\n",
            "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
            " inf inf inf inf inf inf inf inf inf inf]\n",
            "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n",
            "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n",
            "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n",
            "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n",
            "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n",
            "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7SIhbNeTV8b"
      },
      "source": [
        "#AutoGrad with Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vqiG0sqTVQ0"
      },
      "source": [
        "class Layer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super().__init__()\n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "        self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.mm(self.weight)+self.bias.expand(0,len(input.data))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSFiRU9sTg11"
      },
      "source": [
        "class Sequential(Layer):\n",
        "    \n",
        "    def __init__(self, layers=list()):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "    \n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA_pKNpLTjnw"
      },
      "source": [
        "class MSELoss(Layer):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        return ((pred - target)*(pred - target)).sum(0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sivl_Qy9TrkF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "    \n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "            \n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "            \n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if(self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "                    \n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "                if(self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "                \n",
        "                if(self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
        "                \n",
        "                if(self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "                    \n",
        "                if(self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return Tensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(np.tanh(self.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "    \n",
        "    def index_select(self, indices):\n",
        "\n",
        "        if(self.autograd):\n",
        "            new = Tensor(self.data[indices.data],\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "    \n",
        "    def cross_entropy(self, target_indices):\n",
        "\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        \n",
        "        t = target_indices.data.flatten()\n",
        "        p = softmax_output.reshape(len(t),-1)\n",
        "        target_dist = np.eye(p.shape[1])[t]\n",
        "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "    \n",
        "        if(self.autograd):\n",
        "            out = Tensor(loss,\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "        \n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())  \n",
        "    \n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "    \n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Pf3wf_T8YK"
      },
      "source": [
        "class CrossEntropyLoss(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIDoqZIAT05X"
      },
      "source": [
        "class Embedding(Layer):\n",
        "    \n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        \n",
        "        # this random initialiation style is just a convention from word2vec\n",
        "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.weight.index_select(input)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ6XGXKJUGrZ",
        "outputId": "74770180-e8fe-4ff9-91eb-2fe4cb281bbd"
      },
      "source": [
        "import numpy\n",
        "np.random.seed(0)\n",
        "\n",
        "# data indices\n",
        "data = Tensor(np.array(features), autograd=True)\n",
        "\n",
        "# target indices\n",
        "target = Tensor(np.array(new_labels), autograd=True)\n",
        "\n",
        "model = Sequential([Linear(784,784), Tanh(), Linear(784,784)])\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
        "\n",
        "for i in range(10):\n",
        "    \n",
        "    # Predict\n",
        "    pred = model.forward(data)\n",
        "    \n",
        "    # Compare\n",
        "    loss = criterion.forward(pred, target)\n",
        "    \n",
        "    # Learn\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    optim.step()\n",
        "    print(loss)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.356783173893718\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB9yJ9FpNXnF"
      },
      "source": [
        "#Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWodn8eANhRd",
        "outputId": "cb1c2338-643b-4be1-dcfc-0d868238d649"
      },
      "source": [
        "import unittest\n",
        "\n",
        "class TestTensor(unittest.TestCase):\n",
        "\n",
        "    def test_add(self):\n",
        "        x = Tensor([1,2,3,4,5])\n",
        "\n",
        "        y = x + x\n",
        "        self.assertEqual(y.__str__(), \"[ 2  4  6  8 10]\")\n",
        "    def multiple_tensor(self):\n",
        "      a = Tensor([1,2,3,4,5], autograd=True)\n",
        "      b = Tensor([2,2,2,2,2], autograd=True)\n",
        "      c = Tensor([5,4,3,2,1], autograd=True)\n",
        "\n",
        "      d = a + b\n",
        "      e = b + c\n",
        "      f = d + e\n",
        "\n",
        "      f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "\n",
        "      for i in range(len(b.grad.data)):\n",
        "        self.assertEqual(b.grad.data[i], 2) \n",
        "    def test_negation(self):\n",
        "      a = Tensor([1,2,3,4,5], autograd=True)\n",
        "      b = Tensor([2,2,2,2,2], autograd=True)\n",
        "      c = Tensor([5,4,3,2,1], autograd=True)\n",
        "\n",
        "      d = a + (-b)\n",
        "      e = (-b) + c\n",
        "      f = d + e\n",
        "\n",
        "      f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "\n",
        "      for i in range(len(b.grad.data)):\n",
        "        self.assertEqual(b.grad.data[i], -2)\n",
        "\n",
        "    def test_additional_functions(self):\n",
        "      a = Tensor([1,2,3,4,5], autograd=True)\n",
        "      b = Tensor([2,2,2,2,2], autograd=True)\n",
        "      c = Tensor([5,4,3,2,1], autograd=True)\n",
        "\n",
        "      d = a + b\n",
        "      e = b + c\n",
        "      f = d + e\n",
        "\n",
        "      f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "\n",
        "      for i in range(len(b.grad.data)):\n",
        "        self.assertEqual(b.grad.data[i], 2)\n",
        "    \n",
        "    def test_sum_expand(self):\n",
        "      x = Tensor(np.array([[1,2,3], [4,5,6]]))\n",
        "      actual = [5, 7, 9]\n",
        "      predict = x.sum(0)\n",
        "      self.assertEqual(predict.__str__(), \"[5 7 9]\")\n",
        "      actual = [6, 15]\n",
        "      predict = x.sum(1)\n",
        "      self.assertEqual(predict.__str__(), \"[ 6 15]\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.007s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}